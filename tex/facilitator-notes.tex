\documentclass{article}
\usepackage[top=1in,bottom=1in,right=1in,left=1in]{geometry}
\usepackage{enumerate,multicol,amsmath,amssymb}

\begin{document}

\begin{center}
{\Large Facilitator Notes}
\end{center}

\subsection*{Introduction}

These notes are for a one semester, 3-hour sophomore-level Linear Algebra course.  Teams should have at minimum 5 people to ensure a sufficiently sized team factoring in absences.  Ideally, each team should be provided with a vertical,non-permanent surface for their work, such as a whiteboard or chalkboard.  A vertical surface encourages collaboration and discussion, and discourages individual work on paper.  These notes have been field-tested at a large regional public university in classes with 10-35 students, with each ``section'' designed to fill one 50-60 minute class period.  The notes (formatted as slides) are also distributed to the students for use during class and for review purposes; thus, there are ``facts'', ``definitions'', and ``observations'' sprinkled in for reference use, but the bulk of class time is spent on the application activities.  Each application activity has an estimated time listed with it; these add up to 50-60 minutes per day.

The course consists of 5 modules, listed below, as well as a few extra days of applied activities (called Module P). The activities are split into sections optimized for 27 75-minute class days with 60 minutes each day devoted to application activity, and the remaining 15 minutes for assessment.  

\subsubsection*{Module Listing}
\begin{enumerate}
\item [E] Systems of Equations
\item [V] Vector Spaces
\item [A] Algebraic Properties of Linear Maps
\item [M] Algebraic Structure of Matrices
\item [G] Geometric Properties of Linear Maps
\end{enumerate}

\subsection*{Module E: Systems of Linear Equations}
This module explores how to solve arbitrarily large systems of linear equations.  
After introducing some notation, we begin by exploring systems of two equations in two variables, which students learn how to solve (using e.g. back-substitution) in high school algebra.  We quickly move on to infinite solution sets with activity E.9 and begin practicing set-builder notation.  We then begin building fluency in the new language of augmented matrices through activity E.16; here, the instructor is advised to encourage students to think about what the matrix operation means for the corresponding system of equations.  We then motivate the definition of reduced row echelon form as the end goal of the row reduction process. Subsequently, students are introduced to the Gauss-Joradan algorithm of row reduction. Activity E.30 begins getting students familiar with an online computational environment emulating Matlab (we note that other instructional contexts may call for different choices in technology). The module concludes with a series of activities in which students practice solving systems of linear equations. Be sure to encourage students to use technology on these (and all subsequent) activities for matrix reduction. 
 
\subsection*{Module V: Introduction to vector spaces}

This module introduces students to abstract vector spaces and fundamental concepts such as linear independence, spanning sets, and bases.  Activity V.2 motivates the (often seemingly arbitrary, at least to students) definition of a vector space as something that acts like Euclidean space.  After several activities practicing this, we introduce the notions of linear combinations and span. As in Activity V.19, it is important to emphasize that the question ``Is \(\vec{b}\) a linear combination of the vectors \(\vec{v}_1,\ldots,\vec{v}_n\)'' is equivalent to the question ``Is there a solution to the vector equation \(x_1\vec{v_1}+\cdots+x_n\vec{v}_n=\vec{b}\)?''. In our view, it is important that this connection be made explicit, and that students can explain this (rather than answering such a question by blindly appealing to features of an augmented matrix). We also begin practicing with matrix spaces and polynomial spaces at this point. After discussing the natural follow-up question of when a set of vectors spans an entire vector space, we then introduce subspaces.

Starting with Activity V.45, we begin discussing linear independence. Students are generally happy with the informal definition of linear dependence as ``One vector is a linear combination of the others.'' Activity V.47 is designed to help students make the connection between ``One vector is a linear combination of the others'' and ``There is a nontrivial solution to the vector equation \(x_1\vec{v_1}+\cdots+x_n\vec{v}_n=\vec{0}\).''  Again, while we are happy to present Quick Check V.50 to students, our goal is really to have students understand how this connects to the definition, and design our assessments to have students explain this reasoning.

With the concepts of spanning sets and linear independence in hand, it is natural to talk about bases and dimension, first of a vector space in general, and then of a subspace of a vector space. When pressed for time, it can be tempting to skip Activities V.76 and V.77; however, these usually generate lively inter-team discussions as they probe if students really understand the definitions of linear independence and spanning sets. 

 The module concludes with finding bases of homogeneous systems of equations. Be sure to remind students that this will be used when they compute the basis of a kernel of a linear transformation (Standard A3), and then again when finding eigenvectors (Standard G4).



\subsection*{Module A: Algebraic properties of linear maps}
This module introduces the fundamental notion of a linear transformation, and studies them from an algebraic perspective.   
Many students at this level have great difficulty understanding how to show statements involving arbitrary objects; for this reason, in A.3 we begin by walking through an explicit example of showing a map is a linear transformation.  Instructors whose students have completed an introduction to proofs course could likely omit this.  

Activities A.13 through A.16 motivate the idea of the `standard matrix' of a linear transformation. A fundamental idea in this course is that a matrix is, whether you admit it to yourself or not, a linear transformation. We then move on to introducing the kernel and the image of a linear transformation.  The scaffolding in activities A.25-A.27 is crucial (as we have found out the hard way).  In most classes, at least one team will make observation A.33 on their own in the course of the preceding activities.  
Activities A.36 and A.37 outline a proof of the rank-nullity theorem for finite dimensional vector spaces (though we don't assume students have any background in proof writing, so we don't use that language). We then discuss injective and surjective linear maps. 

Activity A.52 is in the spirit of the so-called 'Invertible Matrix Theorem', and is one of our favorites. Be sure to allot at least 15 minutes for the teams to sort the statements, and likely 5-10 minutes for reporting and discussion in classes with more than 4 teams.  We often facilitate reporting on this activity by (after students have sorted the statements into two lists on their whiteboards) asking which statements were easiest to sort; the answer is typically A and F.  Teams will almost always have paired B-E and C-D together, which is our next point of discussion; finally, we move to the most difficult, G and H. 

The module concludes with a discussion of bijective maps. Many classes will deduce Fact A.56 on their own during the discussion of Activities A.54 and A.55.  Activity A.57 serves as a good barometer of where teams are at.  Teams keeping up with everything will, when presented with only part 1, deduce all 3 sections in about a minute; teams that are lagging in their understanding will need closer to 5 minutes for all three parts.  When discussing Activity A.57, instructors are advised to point out how we will use property (c) in the next module to construct inverse maps.  Some classes, due to extended discussion on Activity A.52, do not have time for Activities A.60-A.63; we advise instructors that it is more important to spend the time on A.52.


\subsection*{Module 5-M: Algebraic structure of matrices}
This module introduces students to matrix multiplication and invertible matrices, motivated through the lens of linear transformations.  Many students at this level have seen matrix multiplication in an algorithmic sense (i.e. compute the dot product of rows with columns); while in the past we mentioned this technique, we now avoid this in favor of emphasizing that the raison d'\^{e}tre of matrix multiplication is the composition of linear transformations.  This change in approach led to a doubling in student success on their first assessment of matrix multiplication.

The readiness assurance process is crucial here to make sure students can proceed through Activities M.1-M.4.  Activity M.4 is carefully  scaffolded to encourage students to view matrix multiplication as taking appropriate linear combinations of columns. 
In an ideal world, Activity M.9 should be completed on the first day of the module, but there is rarely time for it in 50 minute meetings.  The bulk of the subsequent activities are devoted to students' discovering that row operations can be viewed as matrix multiplication.  This will be crucial in Module G in developing geometric intuition for the determinant.  We then move on to invertibility.  Activity M.16 is a callback to Activity A.52 and A.57.  After students compile their lists, we recommend asking them to circle the 3 that were easiest to categorize to help facilitate the inter-team discussion.   After the definition of invertibility, Activity M.189 shows students that they already know how to compute the standard matrix of the inverse map.  Some students will view Activity M.24 as a ``cruel trick'' after doing a needless calculation for part 1; we find the ``Aha!'' moment as students discover they did needless work invaluable in cementing the notion of an inverse map as ``undoing'' the original map.

\subsection*{Module G: Geometric properties of linear maps}

This module explores the geometric properties of linear transformations, focusing on determinants, eigenvalues, and eigenvectors.  
The goal of the first sequence of activities is to get students to understand that a determinant is measuring how area/volume changes under a linear transformation.  Activities G.1 through G.9 require the students to make a specific construction of a graph and the resulting area transformation.  Students sometimes have a little trouble with the first of these, but then make short work of the remainder, requiring little class wide discussion.  Activity G.8 invites the first incorrect responses after simultaneous reporting; the instructor is advised to emphasize the multiplicative nature of the stretching factor here.  Activity G.9 can be made a little more concrete by distributing parallelograms and triangles cut out of card stock that invite the students to see the equality geometrically.  Activity G.15 sometimes invites teams to settle on the incorrect answer choice of 7; once again, the instructor is advised to emphasize that the stretching factors (determinants) are multiplicative.

With a definition in hand, we then focus on how row operations affect determinants. Activities G.27 through G.334 help students discover how to compute determinants by using row operations and reducing the dimension. While we briefly mention Laplace expansion, this is typically only used by students who have learned it previously.


The goal of this day is to help students understand how to actually compute a determinant.   Activity G.27 begins an inquiry towards Laplace expansion.  Activities G.27 through G.33 are carefully scaffolded to guide students toward understanding how linearity of the determinant leads to Laplace expansion.  In practice, students will find a mixture of row operations and Laplace expansion the most efficient way to actually perform the computations.

The module and course then concludes with eigenvalues and eigenvectors. When time permits, instructors are encouraged to present some applications, particularly of eigenvalues and eigenvectors, such as Google's Page Rank algorithm.

\end{document}
